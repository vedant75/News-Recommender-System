{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedant75/News-Recommender-System/blob/main/MIND_GNN_Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric torch-scatter torch-sparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvaB9tXg4YWa",
        "outputId": "85847ee0-f7e3-49b6-ab2d-fb6336ad4c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-scatter, torch-sparse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import SAGEConv, GATConv, LGConv, to_hetero\n",
        "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.nn import ModuleList\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration & Helper Functions\n",
        "# ==========================================\n",
        "CONFIG = {\n",
        "    'dataset_url': 'https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip',\n",
        "    'data_dir': './mind_data',\n",
        "    'hidden_channels': 64,\n",
        "    'num_layers': 2,\n",
        "    'epochs': 5,          # Reduced for demo speed (increase to 10-20 for better results)\n",
        "    'batch_size': 1024,   # Batch size for negative sampling\n",
        "    'lr': 0.001,\n",
        "    'model_type': 'GAT',  # Options: 'SAGE', 'GAT', 'LG'\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "}\n",
        "\n",
        "def download_and_extract(url, extract_to):\n",
        "    \"\"\"Downloads and extracts the MIND-small dataset.\"\"\"\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    zip_path = os.path.join(extract_to, 'MINDsmall_train.zip')\n",
        "\n",
        "    # Check if files already exist to avoid re-downloading\n",
        "    if os.path.exists(os.path.join(extract_to, 'behaviors.tsv')) and \\\n",
        "       os.path.exists(os.path.join(extract_to, 'news.tsv')):\n",
        "        print(\"Dataset already exists. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading dataset from {url}...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(zip_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"Download and extraction complete.\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Data Preprocessing\n",
        "# ==========================================\n",
        "def load_mind_data(data_dir):\n",
        "    print(\"Loading raw data...\")\n",
        "    # Load News Data\n",
        "    news_df = pd.read_csv(\n",
        "        os.path.join(data_dir, 'news.tsv'),\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
        "    )\n",
        "\n",
        "    # Load Behaviors Data\n",
        "    behaviors_df = pd.read_csv(\n",
        "        os.path.join(data_dir, 'behaviors.tsv'),\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
        "    )\n",
        "\n",
        "    # --- Mappings ---\n",
        "    # Map News IDs to Integers\n",
        "    unique_news = news_df['news_id'].unique()\n",
        "    news_id_map = {nid: i for i, nid in enumerate(unique_news)}\n",
        "    num_news = len(unique_news)\n",
        "\n",
        "    # Map User IDs to Integers\n",
        "    unique_users = behaviors_df['user_id'].unique()\n",
        "    user_id_map = {uid: i for i, uid in enumerate(unique_users)}\n",
        "    num_users = len(unique_users)\n",
        "\n",
        "    # Map Categories to Integers (for features)\n",
        "    unique_categories = news_df['category'].unique()\n",
        "    category_map = {cat: i for i, cat in enumerate(unique_categories)}\n",
        "    num_categories = len(unique_categories)\n",
        "\n",
        "    # --- Node Features ---\n",
        "    # News Features: One-Hot Encoding of Category\n",
        "    news_features = torch.zeros((num_news, num_categories))\n",
        "    for _, row in news_df.iterrows():\n",
        "        if row['news_id'] in news_id_map:\n",
        "            nid_idx = news_id_map[row['news_id']]\n",
        "            cat_idx = category_map[row['category']]\n",
        "            news_features[nid_idx, cat_idx] = 1.0\n",
        "\n",
        "    # --- Edge Construction ---\n",
        "    # We collect edges from the 'history' column (user clicked these in the past)\n",
        "    # Note: For a true recommendation task, we might also parse 'impressions',\n",
        "    # but 'history' provides the core user profile graph.\n",
        "\n",
        "    src = []\n",
        "    dst = []\n",
        "\n",
        "    print(\"Processing interaction history to build graph...\")\n",
        "    # Iterate through behaviors to build edges\n",
        "    # Dropping NaNs in history\n",
        "    behaviors_df = behaviors_df.dropna(subset=['history'])\n",
        "\n",
        "    for _, row in behaviors_df.iterrows():\n",
        "        u_idx = user_id_map[row['user_id']]\n",
        "        history_str = str(row['history'])\n",
        "        clicked_news_ids = history_str.split()\n",
        "\n",
        "        for news_id in clicked_news_ids:\n",
        "            if news_id in news_id_map:\n",
        "                n_idx = news_id_map[news_id]\n",
        "                src.append(u_idx)\n",
        "                dst.append(n_idx)\n",
        "\n",
        "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
        "\n",
        "    # --- Create HeteroData Object ---\n",
        "    data = HeteroData()\n",
        "\n",
        "    # Add Nodes\n",
        "    data['user'].num_nodes = num_users\n",
        "    # User features: We will use an Embedding layer in the model, so we don't strictly need static features here,\n",
        "    # but PyG transforms often require a 'x' or 'num_nodes'.\n",
        "    # Let's add a dummy feature or just rely on num_nodes.\n",
        "    # To keep it compatible with the blog's \"x_dict['user'] = self.user_emb(data['user'].node_id)\",\n",
        "    # we add a node_id vector.\n",
        "    data['user'].node_id = torch.arange(num_users)\n",
        "\n",
        "    data['news'].num_nodes = num_news\n",
        "    data['news'].x = news_features # Category one-hot\n",
        "    data['news'].node_id = torch.arange(num_news)\n",
        "\n",
        "    # Add Edges\n",
        "    data['user', 'clicks', 'news'].edge_index = edge_index\n",
        "\n",
        "    return data, num_users, num_news, user_id_map, news_id_map\n",
        "\n",
        "# ==========================================\n",
        "# 3. Model Architecture\n",
        "# ==========================================\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, hidden_channels, num_layers, conv_type):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.conv_type = conv_type\n",
        "\n",
        "        if conv_type == \"SAGE\":\n",
        "            self.convs = ModuleList(SAGEConv(hidden_channels, hidden_channels) for _ in range(num_layers))\n",
        "        elif conv_type == \"GAT\":\n",
        "            # add_self_loops=False is important for bipartite graphs in some PyG versions\n",
        "            self.convs = ModuleList(GATConv(hidden_channels, hidden_channels, heads=2, concat=False, add_self_loops=False) for _ in range(num_layers))\n",
        "        elif conv_type == \"LG\":\n",
        "            self.convs = ModuleList(LGConv() for _ in range(num_layers))\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        # Dictionary unpacking for Heterogeneous Graphs\n",
        "\n",
        "        if self.conv_type == \"LG\":\n",
        "            # LGConv expects a homogeneous graph structure logic manually applied\n",
        "            x_user = x_dict['user']\n",
        "            x_news = x_dict['news']\n",
        "\n",
        "            # Create a virtual homogeneous node feature set\n",
        "            x = torch.cat([x_user, x_news], dim=0)\n",
        "\n",
        "            # Map heterogeneous edges to homogeneous indices\n",
        "            # User indices [0, num_users-1]\n",
        "            # News indices [num_users, num_users + num_news - 1]\n",
        "            edge_index_user_news = edge_index_dict[('user', 'clicks', 'news')]\n",
        "\n",
        "            # Offset news indices\n",
        "            num_users = x_user.size(0)\n",
        "            src = edge_index_user_news[0]\n",
        "            dst = edge_index_user_news[1] + num_users\n",
        "\n",
        "            # Bi-directional edges for LightGCN propagation\n",
        "            edge_index = torch.cat([\n",
        "                torch.stack([src, dst], dim=0),\n",
        "                torch.stack([dst, src], dim=0)\n",
        "            ], dim=1)\n",
        "\n",
        "            for i in range(self.num_layers):\n",
        "                x = self.convs[i](x, edge_index)\n",
        "                # LightGCN usually doesn't use activation functions between layers\n",
        "\n",
        "            # Split back\n",
        "            x_user_out = x[:num_users]\n",
        "            x_news_out = x[num_users:]\n",
        "\n",
        "            return {'user': x_user_out, 'news': x_news_out}\n",
        "\n",
        "        else:\n",
        "            # SAGE and GAT handled via to_hetero wrapper typically,\n",
        "            # but here we define the inner logic which to_hetero will convert.\n",
        "            # NOTE: If we use to_hetero, the input x and edge_index are specific to types.\n",
        "            # However, inside the ModuleList loop, standard PyG convs expect standard tensors.\n",
        "            # The `to_hetero` transformation happens in the main Model class.\n",
        "\n",
        "            # This 'forward' is technically just for the structure.\n",
        "            # When wrapped with to_hetero, it expects x_dict and edge_index_dict automatically?\n",
        "            # Actually, standard to_hetero usage involves defining a standard GNN and passing it.\n",
        "            # But the blog defines a custom forward for LG.\n",
        "\n",
        "            # Let's simplify: Return the convs. `to_hetero` will patch the module.\n",
        "            # We just need to define how *one* layer works.\n",
        "            # But the blog had the loop inside forward.\n",
        "\n",
        "            # We will follow standard PyG + to_hetero pattern:\n",
        "            # Define a model that takes (x, edge_index), then convert it.\n",
        "            pass\n",
        "\n",
        "class StandardGNN(nn.Module):\n",
        "    \"\"\"Standard GNN for SAGE/GAT to be converted by to_hetero\"\"\"\n",
        "    def __init__(self, hidden_channels, num_layers, conv_type):\n",
        "        super().__init__()\n",
        "        self.conv_type = conv_type\n",
        "        self.num_layers = num_layers\n",
        "        if conv_type == \"SAGE\":\n",
        "            self.convs = ModuleList(SAGEConv(hidden_channels, hidden_channels) for _ in range(num_layers))\n",
        "        elif conv_type == \"GAT\":\n",
        "            self.convs = ModuleList(GATConv(hidden_channels, hidden_channels, heads=2, concat=False, add_self_loops=False) for _ in range(num_layers))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            if i < self.num_layers - 1:\n",
        "                x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def forward(self, x_user: torch.Tensor, x_news: torch.Tensor, edge_label_index: torch.Tensor) -> torch.Tensor:\n",
        "        # edge_label_index[0] are user indices, edge_label_index[1] are news indices\n",
        "        edge_feat_user = x_user[edge_label_index[0]]\n",
        "        edge_feat_news = x_news[edge_label_index[1]]\n",
        "        # Dot product\n",
        "        return (edge_feat_user * edge_feat_news).sum(dim=-1)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, hidden_channels, data, num_layers=2, conv_type=\"SAGE\"):\n",
        "        super().__init__()\n",
        "        self.conv_type = conv_type\n",
        "\n",
        "        # Initial Embeddings\n",
        "        # News: Linear projection of category One-Hot + Learnable Embedding ID\n",
        "        self.news_lin = nn.Linear(data['news'].x.size(1), hidden_channels)\n",
        "        self.news_emb = nn.Embedding(data['news'].num_nodes, hidden_channels)\n",
        "\n",
        "        # User: Learnable Embedding\n",
        "        self.user_emb = nn.Embedding(data['user'].num_nodes, hidden_channels)\n",
        "\n",
        "        # GNN Backbone\n",
        "        if conv_type == \"LG\":\n",
        "            self.gnn = GNN(hidden_channels, num_layers, conv_type)\n",
        "        else:\n",
        "            # Use StandardGNN and convert to hetero\n",
        "            base_gnn = StandardGNN(hidden_channels, num_layers, conv_type)\n",
        "            self.gnn = to_hetero(base_gnn, data.metadata(), aggr='mean')\n",
        "\n",
        "        self.classifier = Classifier()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x_dict = self.get_embeddings(data)\n",
        "        # Predict on specific edges (edge_label_index)\n",
        "        # Note: 'edge_label_index' comes from the LinkSplit transform\n",
        "        pred = self.classifier(\n",
        "            x_dict['user'],\n",
        "            x_dict['news'],\n",
        "            data['user', 'clicks', 'news'].edge_label_index\n",
        "        )\n",
        "        return pred\n",
        "\n",
        "    def get_embeddings(self, data):\n",
        "        x_news = self.news_lin(data['news'].x) + self.news_emb(data['news'].node_id)\n",
        "        x_user = self.user_emb(data['user'].node_id)\n",
        "\n",
        "        x_dict = {'user': x_user, 'news': x_news}\n",
        "\n",
        "        # Apply GNN\n",
        "        if self.conv_type == \"LG\":\n",
        "            x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
        "        else:\n",
        "            x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
        "\n",
        "        return x_dict\n",
        "\n",
        "def bpr_loss(user_emb, news_emb, pos_edge_index, neg_edge_index):\n",
        "    # Calculate scores for positive edges\n",
        "    pos_scores = (user_emb[pos_edge_index[0]] * news_emb[pos_edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "    # Calculate scores for negative edges\n",
        "    # Check bounds to ensure we don't crash if batches differ slightly\n",
        "    min_len = min(pos_scores.size(0), neg_edge_index.size(1))\n",
        "    pos_scores = pos_scores[:min_len]\n",
        "    neg_edge_index = neg_edge_index[:, :min_len]\n",
        "\n",
        "    neg_scores = (user_emb[neg_edge_index[0]] * news_emb[neg_edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "    # BPR Loss formula: -log(sigmoid(pos - neg))\n",
        "    loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-15).mean()\n",
        "    return loss\n",
        "\n",
        "def random_negative_sampling(edge_index, num_news, num_neg_samples=None):\n",
        "    users = edge_index[0]\n",
        "    if num_neg_samples is None:\n",
        "        num_neg_samples = users.size(0)\n",
        "\n",
        "    # Randomly sample news items as negatives\n",
        "    neg_news = torch.randint(0, num_news, (num_neg_samples,), device=edge_index.device)\n",
        "\n",
        "    neg_edge_index = torch.stack([users, neg_news], dim=0)\n",
        "    return neg_edge_index\n",
        "\n",
        "# ==========================================\n",
        "# 4. Main Execution\n",
        "# ==========================================\n",
        "def main():\n",
        "    print(\"--- 1. Downloading Data ---\")\n",
        "    download_and_extract(CONFIG['dataset_url'], CONFIG['data_dir'])\n",
        "\n",
        "    print(\"--- 2. Processing Data ---\")\n",
        "    data, num_users, num_news, user_map, news_map = load_mind_data(CONFIG['data_dir'])\n",
        "\n",
        "    # Convert to undirected for message passing (users affect news, news affect users)\n",
        "    # But for recommendation target, we want User -> News.\n",
        "    # PyG LinkSplit handles this.\n",
        "\n",
        "    print(\"--- 3. Splitting Data (Train/Val/Test) ---\")\n",
        "    # RandomLinkSplit splits the 'edge_index' into train/val/test sets\n",
        "    # and creates 'edge_label_index' for prediction.\n",
        "    transform = RandomLinkSplit(\n",
        "        num_val=0.1,\n",
        "        num_test=0.1,\n",
        "        neg_sampling_ratio=1.0, # Generates 1 negative for every positive in val/test\n",
        "        add_negative_train_samples=False, # We do this manually in training for BPR\n",
        "        edge_types=[('user', 'clicks', 'news')],\n",
        "        rev_edge_types=[('news', 'rev_clicks', 'user')]\n",
        "    )\n",
        "\n",
        "    train_data, val_data, test_data = transform(data)\n",
        "\n",
        "    device = CONFIG['device']\n",
        "    train_data = train_data.to(device)\n",
        "    val_data = val_data.to(device)\n",
        "    test_data = test_data.to(device)\n",
        "\n",
        "    print(f\"--- 4. Initializing {CONFIG['model_type']} Model ---\")\n",
        "    model = Model(\n",
        "        hidden_channels=CONFIG['hidden_channels'],\n",
        "        data=data,\n",
        "        num_layers=CONFIG['num_layers'],\n",
        "        conv_type=CONFIG['model_type']\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])\n",
        "\n",
        "    print(\"--- 5. Training Loop ---\")\n",
        "    for epoch in range(1, CONFIG['epochs'] + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Get embeddings\n",
        "        x_dict = model.get_embeddings(train_data)\n",
        "\n",
        "        # Positive edges from training set\n",
        "        pos_edge_index = train_data['user', 'clicks', 'news'].edge_label_index\n",
        "\n",
        "        # Negative sampling (create random edges that don't exist)\n",
        "        neg_edge_index = random_negative_sampling(\n",
        "            pos_edge_index,\n",
        "            num_news,\n",
        "            num_neg_samples=pos_edge_index.size(1)\n",
        "        )\n",
        "\n",
        "        # Compute Loss (BPR)\n",
        "        loss = bpr_loss(x_dict['user'], x_dict['news'], pos_edge_index, neg_edge_index)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Get predictions for validation set (which includes pos and neg edges from LinkSplit)\n",
        "            val_preds = model(val_data)\n",
        "            val_labels = val_data['user', 'clicks', 'news'].edge_label\n",
        "\n",
        "            auc = roc_auc_score(val_labels.cpu().numpy(), val_preds.sigmoid().cpu().numpy())\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Loss: {loss.item():.4f} | Val AUC: {auc:.4f}\")\n",
        "\n",
        "    print(\"--- 6. Final Evaluation (Test Set) ---\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_preds = model(test_data)\n",
        "        test_labels = test_data['user', 'clicks', 'news'].edge_label\n",
        "        auc = roc_auc_score(test_labels.cpu().numpy(), test_preds.sigmoid().cpu().numpy())\n",
        "\n",
        "    print(f\"Test AUC: {auc:.4f}\")\n",
        "\n",
        "    # --- Recall@K Calculation ---\n",
        "    print(\"--- 7. Generating Top-K Recommendations ---\")\n",
        "    # Helper to calculate Recall@K for a few users\n",
        "    k = 10\n",
        "    user_indices = test_data['user'].node_id[:10] # Sample 10 users\n",
        "\n",
        "    embeddings = model.get_embeddings(test_data)\n",
        "    user_emb = embeddings['user']\n",
        "    news_emb = embeddings['news']\n",
        "\n",
        "    # Compute similarity matrix for all users (careful with memory on large data)\n",
        "    # Here we do it for the sampled users only\n",
        "    sample_user_emb = user_emb[user_indices]\n",
        "    scores = torch.matmul(sample_user_emb, news_emb.t()) # [10, num_news]\n",
        "\n",
        "    _, top_indices = torch.topk(scores, k=k)\n",
        "\n",
        "    # Reverse map news IDs to titles\n",
        "    # Load news DF again or use existing if in memory (it is)\n",
        "    # We need a map Int -> Title\n",
        "    news_df = pd.read_csv(\n",
        "        os.path.join(CONFIG['data_dir'], 'news.tsv'),\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
        "    )\n",
        "    # Make a quick lookup\n",
        "    int_to_newsid = {v: k for k, v in news_map.items()}\n",
        "    newsid_to_title = pd.Series(news_df.title.values, index=news_df.news_id).to_dict()\n",
        "\n",
        "    print(f\"\\nTop {k} Recommendations for sample users:\")\n",
        "    for i, user_idx in enumerate(user_indices):\n",
        "        print(f\"\\nUser Index: {user_idx.item()}\")\n",
        "        recs = top_indices[i].cpu().numpy()\n",
        "        for news_idx in recs:\n",
        "            nid = int_to_newsid.get(news_idx, \"Unknown\")\n",
        "            title = newsid_to_title.get(nid, \"Unknown Title\")\n",
        "            print(f\" - [{nid}] {title[:50]}...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "WFsJC-dC4VEx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}