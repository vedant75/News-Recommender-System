{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedant75/News-Recommender-System/blob/main/MIND_GNN_Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n",
        "# !pip install torch-scatter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvaB9tXg4YWa",
        "outputId": "3dcfaba2-8cbc-47e4-9109-cf4013fcd67d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-sparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g908AASt8aFq",
        "outputId": "a9a11150-af30-4e94-ba30-3e2037234903"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp312-cp312-linux_x86_64.whl size=3039787 sha256=7752a64e3ab178ddc0d388dc10dd9f67ce50c0503e6d24732cc9e42afbfc4b73\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/fa/21/bd1d78ce1629aec4ecc924a63b82f6949dda484b6321eac6f2\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import SAGEConv, GATConv, LGConv, to_hetero\n",
        "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.nn import ModuleList\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration & Helper Functions\n",
        "# ==========================================\n",
        "CONFIG = {\n",
        "    'dataset_url': 'https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip',\n",
        "    'data_dir': './mind_data',\n",
        "    'hidden_channels': 64,\n",
        "    'num_layers': 2,\n",
        "    'epochs': 20,         # Increased to 20 to match blog convergence\n",
        "    'batch_size': 1024,   # Batch size for negative sampling\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 1e-5,\n",
        "    'model_type': 'GAT',  # Blog winner: GAT\n",
        "    'heads': 4,           # Blog specified 4 heads for GAT\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "}\n",
        "\n",
        "def download_and_extract(url, extract_to):\n",
        "    \"\"\"Downloads and extracts the MIND-small dataset.\"\"\"\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    zip_path = os.path.join(extract_to, 'MINDsmall_train.zip')\n",
        "\n",
        "    # Check if files already exist to avoid re-downloading\n",
        "    if os.path.exists(os.path.join(extract_to, 'behaviors.tsv')) and \\\n",
        "       os.path.exists(os.path.join(extract_to, 'news.tsv')):\n",
        "        print(\"Dataset already exists. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading dataset from {url}...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    # Check if the download was successful\n",
        "    response.raise_for_status()\n",
        "    with open(zip_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"Download and extraction complete.\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Data Preprocessing\n",
        "# ==========================================\n",
        "def load_mind_data(data_dir):\n",
        "    print(\"Loading raw data...\")\n",
        "    # Load News Data\n",
        "    news_df = pd.read_csv(\n",
        "        os.path.join(data_dir, 'news.tsv'),\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
        "    )\n",
        "\n",
        "    # Load Behaviors Data\n",
        "    behaviors_df = pd.read_csv(\n",
        "        os.path.join(data_dir, 'behaviors.tsv'),\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
        "    )\n",
        "\n",
        "    # --- Mappings ---\n",
        "    # Map News IDs to Integers\n",
        "    unique_news = news_df['news_id'].unique()\n",
        "    news_id_map = {nid: i for i, nid in enumerate(unique_news)}\n",
        "    num_news = len(unique_news)\n",
        "\n",
        "    # Map User IDs to Integers\n",
        "    unique_users = behaviors_df['user_id'].unique()\n",
        "    user_id_map = {uid: i for i, uid in enumerate(unique_users)}\n",
        "    num_users = len(unique_users)\n",
        "\n",
        "    # Map Categories to Integers (for features)\n",
        "    unique_categories = news_df['category'].unique()\n",
        "    category_map = {cat: i for i, cat in enumerate(unique_categories)}\n",
        "    num_categories = len(unique_categories)\n",
        "\n",
        "    # --- Node Features ---\n",
        "    # News Features: One-Hot Encoding of Category\n",
        "    news_features = torch.zeros((num_news, num_categories))\n",
        "    for _, row in news_df.iterrows():\n",
        "        if row['news_id'] in news_id_map:\n",
        "            nid_idx = news_id_map[row['news_id']]\n",
        "            cat_idx = category_map[row['category']]\n",
        "            news_features[nid_idx, cat_idx] = 1.0\n",
        "\n",
        "    # --- Edge Construction ---\n",
        "    src = []\n",
        "    dst = []\n",
        "\n",
        "    print(\"Processing interaction history to build graph...\")\n",
        "    behaviors_df = behaviors_df.dropna(subset=['history'])\n",
        "\n",
        "    for _, row in behaviors_df.iterrows():\n",
        "        u_idx = user_id_map[row['user_id']]\n",
        "        history_str = str(row['history'])\n",
        "        clicked_news_ids = history_str.split()\n",
        "\n",
        "        for news_id in clicked_news_ids:\n",
        "            if news_id in news_id_map:\n",
        "                n_idx = news_id_map[news_id]\n",
        "                src.append(u_idx)\n",
        "                dst.append(n_idx)\n",
        "\n",
        "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
        "\n",
        "    # --- Create HeteroData Object ---\n",
        "    data = HeteroData()\n",
        "    data['user'].num_nodes = num_users\n",
        "    data['user'].node_id = torch.arange(num_users)\n",
        "    data['news'].num_nodes = num_news\n",
        "    data['news'].x = news_features\n",
        "    data['news'].node_id = torch.arange(num_news)\n",
        "    data['user', 'clicks', 'news'].edge_index = edge_index\n",
        "\n",
        "    return data, num_users, num_news, user_id_map, news_id_map\n",
        "\n",
        "# ==========================================\n",
        "# 3. Model Architecture\n",
        "# ==========================================\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, hidden_channels, num_layers, conv_type):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.conv_type = conv_type\n",
        "\n",
        "        if conv_type == \"SAGE\":\n",
        "            self.convs = ModuleList(SAGEConv(hidden_channels, hidden_channels) for _ in range(num_layers))\n",
        "        elif conv_type == \"GAT\":\n",
        "            # Match blog: heads=4, concat=False (averaging heads usually better for embedding size stability)\n",
        "            self.convs = ModuleList(GATConv(hidden_channels, hidden_channels, heads=CONFIG['heads'], concat=False, add_self_loops=False) for _ in range(num_layers))\n",
        "        elif conv_type == \"LG\":\n",
        "            self.convs = ModuleList(LGConv() for _ in range(num_layers))\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        if self.conv_type == \"LG\":\n",
        "            # Manual Homogeneous conversion for LightGCN\n",
        "            x_user = x_dict['user']\n",
        "            x_news = x_dict['news']\n",
        "            x = torch.cat([x_user, x_news], dim=0)\n",
        "\n",
        "            edge_index_user_news = edge_index_dict[('user', 'clicks', 'news')]\n",
        "            num_users = x_user.size(0)\n",
        "            src = edge_index_user_news[0]\n",
        "            dst = edge_index_user_news[1] + num_users\n",
        "\n",
        "            edge_index = torch.cat([\n",
        "                torch.stack([src, dst], dim=0),\n",
        "                torch.stack([dst, src], dim=0)\n",
        "            ], dim=1)\n",
        "\n",
        "            for i in range(self.num_layers):\n",
        "                x = self.convs[i](x, edge_index)\n",
        "\n",
        "            x_user_out = x[:num_users]\n",
        "            x_news_out = x[num_users:]\n",
        "            return {'user': x_user_out, 'news': x_news_out}\n",
        "        else:\n",
        "            # Placeholder for to_hetero (not called directly)\n",
        "            pass\n",
        "\n",
        "class StandardGNN(nn.Module):\n",
        "    \"\"\"Standard GNN Wrapper for SAGE/GAT to be converted by to_hetero\"\"\"\n",
        "    def __init__(self, hidden_channels, num_layers, conv_type):\n",
        "        super().__init__()\n",
        "        self.conv_type = conv_type\n",
        "        self.num_layers = num_layers\n",
        "        if conv_type == \"SAGE\":\n",
        "            self.convs = ModuleList(SAGEConv(hidden_channels, hidden_channels) for _ in range(num_layers))\n",
        "        elif conv_type == \"GAT\":\n",
        "             self.convs = ModuleList(GATConv(hidden_channels, hidden_channels, heads=CONFIG['heads'], concat=False, add_self_loops=False) for _ in range(num_layers))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            if i < self.num_layers - 1:\n",
        "                x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def forward(self, x_user: torch.Tensor, x_news: torch.Tensor, edge_label_index: torch.Tensor) -> torch.Tensor:\n",
        "        edge_feat_user = x_user[edge_label_index[0]]\n",
        "        edge_feat_news = x_news[edge_label_index[1]]\n",
        "        return (edge_feat_user * edge_feat_news).sum(dim=-1)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, hidden_channels, data, num_layers=2, conv_type=\"SAGE\"):\n",
        "        super().__init__()\n",
        "        self.conv_type = conv_type\n",
        "\n",
        "        self.news_lin = nn.Linear(data['news'].x.size(1), hidden_channels)\n",
        "        self.news_emb = nn.Embedding(data['news'].num_nodes, hidden_channels)\n",
        "        self.user_emb = nn.Embedding(data['user'].num_nodes, hidden_channels)\n",
        "\n",
        "        if conv_type == \"LG\":\n",
        "            self.gnn = GNN(hidden_channels, num_layers, conv_type)\n",
        "        else:\n",
        "            base_gnn = StandardGNN(hidden_channels, num_layers, conv_type)\n",
        "            self.gnn = to_hetero(base_gnn, data.metadata(), aggr='mean')\n",
        "\n",
        "        self.classifier = Classifier()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x_dict = self.get_embeddings(data)\n",
        "        pred = self.classifier(\n",
        "            x_dict['user'],\n",
        "            x_dict['news'],\n",
        "            data['user', 'clicks', 'news'].edge_label_index\n",
        "        )\n",
        "        return pred\n",
        "\n",
        "    def get_embeddings(self, data):\n",
        "        x_news = self.news_lin(data['news'].x) + self.news_emb(data['news'].node_id)\n",
        "        x_user = self.user_emb(data['user'].node_id)\n",
        "        x_dict = {'user': x_user, 'news': x_news}\n",
        "\n",
        "        # Apply GNN\n",
        "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
        "        return x_dict\n",
        "\n",
        "# Updated BPR Loss to match blog (Added regularization)\n",
        "def bpr_loss(user_emb, news_emb, pos_edge_index, neg_edge_index, lambda_reg=1e-4):\n",
        "    pos_scores = (user_emb[pos_edge_index[0]] * news_emb[pos_edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "    min_len = min(pos_scores.size(0), neg_edge_index.size(1))\n",
        "    pos_scores = pos_scores[:min_len]\n",
        "    neg_edge_index = neg_edge_index[:, :min_len]\n",
        "\n",
        "    neg_scores = (user_emb[neg_edge_index[0]] * news_emb[neg_edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "    # BPR Loss + Regularization\n",
        "    loss_bpr = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-15).mean()\n",
        "    loss_reg = lambda_reg * (user_emb[pos_edge_index[0]].norm(2).pow(2) +\n",
        "                             news_emb[pos_edge_index[1]].norm(2).pow(2)) / float(min_len)\n",
        "\n",
        "    return loss_bpr + loss_reg\n",
        "\n",
        "def random_negative_sampling(edge_index, num_news, num_neg_samples=None):\n",
        "    users = edge_index[0]\n",
        "    if num_neg_samples is None:\n",
        "        num_neg_samples = users.size(0)\n",
        "    neg_news = torch.randint(0, num_news, (num_neg_samples,), device=edge_index.device)\n",
        "    neg_edge_index = torch.stack([users, neg_news], dim=0)\n",
        "    return neg_edge_index\n",
        "\n",
        "# ==========================================\n",
        "# 4. Main Execution\n",
        "# ==========================================\n",
        "def main():\n",
        "    print(\"--- 1. Downloading Data ---\")\n",
        "    download_and_extract(CONFIG['dataset_url'], CONFIG['data_dir'])\n",
        "\n",
        "    print(\"--- 2. Processing Data ---\")\n",
        "    data, num_users, num_news, user_map, news_map = load_mind_data(CONFIG['data_dir'])\n",
        "\n",
        "    print(\"--- 3. Splitting Data (Train/Val/Test) ---\")\n",
        "    transform = RandomLinkSplit(\n",
        "        num_val=0.1,\n",
        "        num_test=0.1,\n",
        "        neg_sampling_ratio=1.0,\n",
        "        add_negative_train_samples=False,\n",
        "        edge_types=[('user', 'clicks', 'news')],\n",
        "        rev_edge_types=[('news', 'rev_clicks', 'user')]\n",
        "    )\n",
        "\n",
        "    train_data, val_data, test_data = transform(data)\n",
        "\n",
        "    device = CONFIG['device']\n",
        "    train_data = train_data.to(device)\n",
        "    val_data = val_data.to(device)\n",
        "    test_data = test_data.to(device)\n",
        "\n",
        "    print(f\"--- 4. Initializing {CONFIG['model_type']} Model (Heads={CONFIG['heads']}) ---\")\n",
        "    model = Model(\n",
        "        hidden_channels=CONFIG['hidden_channels'],\n",
        "        data=data,\n",
        "        num_layers=CONFIG['num_layers'],\n",
        "        conv_type=CONFIG['model_type']\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
        "\n",
        "    print(f\"--- 5. Training Loop ({CONFIG['epochs']} Epochs) ---\")\n",
        "    for epoch in range(1, CONFIG['epochs'] + 1):\n",
        "        model.train()\n",
        "\n",
        "        x_dict = model.get_embeddings(train_data)\n",
        "        pos_edge_index = train_data['user', 'clicks', 'news'].edge_label_index\n",
        "        neg_edge_index = random_negative_sampling(\n",
        "            pos_edge_index,\n",
        "            num_news,\n",
        "            num_neg_samples=pos_edge_index.size(1)\n",
        "        )\n",
        "\n",
        "        loss = bpr_loss(x_dict['user'], x_dict['news'], pos_edge_index, neg_edge_index)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = model(val_data)\n",
        "            val_labels = val_data['user', 'clicks', 'news'].edge_label\n",
        "            auc = roc_auc_score(val_labels.cpu().numpy(), val_preds.sigmoid().cpu().numpy())\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Loss: {loss.item():.4f} | Val AUC: {auc:.4f}\")\n",
        "\n",
        "    print(\"--- 6. Final Evaluation (Test Set) ---\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_preds = model(test_data)\n",
        "        test_labels = test_data['user', 'clicks', 'news'].edge_label\n",
        "        auc = roc_auc_score(test_labels.cpu().numpy(), test_preds.sigmoid().cpu().numpy())\n",
        "\n",
        "    print(f\"Test AUC: {auc:.4f}\")\n",
        "\n",
        "    # --- Recall@1000 Calculation (Batched) ---\n",
        "    print(\"--- 7. Generating Top-K Recommendations (Recall@1000) ---\")\n",
        "    k = 1000\n",
        "    # Calculate for a subset of users to demonstrate functionality\n",
        "    # Calculating for ALL 50k users might timeout on free Colab\n",
        "    num_eval_users = 100\n",
        "    user_indices = test_data['user'].node_id[:num_eval_users]\n",
        "\n",
        "    embeddings = model.get_embeddings(test_data)\n",
        "    user_emb = embeddings['user'][user_indices]\n",
        "    news_emb = embeddings['news']\n",
        "\n",
        "    recall_hits = 0\n",
        "\n",
        "    # Batch processing for similarity to avoid OOM\n",
        "    batch_size = 10 # Process 10 users at a time\n",
        "    print(f\"Calculating Recall@{k} for {num_eval_users} users...\")\n",
        "\n",
        "    for i in range(0, num_eval_users, batch_size):\n",
        "        end = min(i + batch_size, num_eval_users)\n",
        "        batch_user_emb = user_emb[i:end]\n",
        "\n",
        "        # Matrix multiplication: [Batch, Hidden] @ [Hidden, News] -> [Batch, News]\n",
        "        scores = torch.matmul(batch_user_emb, news_emb.t())\n",
        "\n",
        "        # Get Top K\n",
        "        _, top_indices = torch.topk(scores, k=k)\n",
        "\n",
        "        # Check against ground truth (Test Edges)\n",
        "        # Note: In a real rigorous recall test, we must group ground truth by user.\n",
        "        # This is a simplified check against the test edge set provided by LinkSplit\n",
        "        # For a full Recall metric, we need a dictionary of {user: [ground_truth_items]}\n",
        "        # We will build that quickly for these users\n",
        "\n",
        "        current_user_ids = user_indices[i:end].cpu().numpy()\n",
        "\n",
        "        # Filter test edges for these users\n",
        "        test_edges = test_data['user', 'clicks', 'news'].edge_label_index\n",
        "        test_labels = test_data['user', 'clicks', 'news'].edge_label\n",
        "        # We only care about positive edges in test set\n",
        "        pos_mask = test_labels == 1\n",
        "        pos_test_edges = test_edges[:, pos_mask]\n",
        "\n",
        "        for idx, u_id in enumerate(current_user_ids):\n",
        "            # Get true items for this user in test set\n",
        "            # (Note: This search is slow, in production use a pre-built dict)\n",
        "            mask = pos_test_edges[0] == u_id\n",
        "            true_items = pos_test_edges[1][mask].cpu().numpy()\n",
        "\n",
        "            if len(true_items) == 0:\n",
        "                continue\n",
        "\n",
        "            # Check overlap\n",
        "            recs = top_indices[idx].cpu().numpy()\n",
        "            hits = np.intersect1d(true_items, recs)\n",
        "            if len(hits) > 0:\n",
        "                recall_hits += 1\n",
        "\n",
        "    recall_score = recall_hits / num_eval_users\n",
        "    print(f\"Recall@{k} (Sampled): {recall_score:.4f}\")\n",
        "\n",
        "    # Show actual recommendations for the first user in the batch\n",
        "    news_df = pd.read_csv(\n",
        "        os.path.join(CONFIG['data_dir'], 'news.tsv'),\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
        "    )\n",
        "    int_to_newsid = {v: k for k, v in news_map.items()}\n",
        "    newsid_to_title = pd.Series(news_df.title.values, index=news_df.news_id).to_dict()\n",
        "\n",
        "    print(f\"\\nTop 10 Recommendations for User {user_indices[0].item()}:\")\n",
        "    recs = top_indices[0].cpu().numpy()[:10]\n",
        "    for news_idx in recs:\n",
        "        nid = int_to_newsid.get(news_idx, \"Unknown\")\n",
        "        title = newsid_to_title.get(nid, \"Unknown Title\")\n",
        "        print(f\" - [{nid}] {title[:60]}...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Downloading Data ---\n",
            "Downloading dataset from https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "409 Client Error: Public access is not permitted on this storage account. for url: https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4272221238.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4272221238.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- 1. Downloading Data ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- 2. Processing Data ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4272221238.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[0;34m(url, extract_to)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Check if the download was successful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 409 Client Error: Public access is not permitted on this storage account. for url: https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "id": "WFsJC-dC4VEx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "3234c6fd-1237-49c7-fef2-fabb9dc952c3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}